{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load libraries"
      ],
      "metadata": {
        "id": "9J2E83OudZwu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toMwHh00osII"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization function\n",
        "def normalize_within_range(value, lower_bound=0, higher_bound=1):\n",
        "    return (np.clip(value - lower_bound, 0, None)) / (higher_bound - lower_bound) * 100"
      ],
      "metadata": {
        "id": "GzZwAmDLpZy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbh_subtasks = {\n",
        "    \"sports_understanding\": 2,\n",
        "    \"tracking_shuffled_objects_three_objects\": 3,\n",
        "    \"navigate\": 2,\n",
        "    \"snarks\": 2,\n",
        "    \"date_understanding\": 6,\n",
        "    \"reasoning_about_colored_objects\": 18,\n",
        "    \"object_counting\": 19,\n",
        "    \"logical_deduction_seven_objects\": 7,\n",
        "    \"geometric_shapes\": 11,\n",
        "    \"web_of_lies\": 2,\n",
        "    \"movie_recommendation\": 6,\n",
        "    \"logical_deduction_five_objects\": 5,\n",
        "    \"salient_translation_error_detection\": 6,\n",
        "    \"disambiguation_qa\": 3,\n",
        "    \"temporal_sequences\": 4,\n",
        "    \"hyperbaton\": 2,\n",
        "    \"logical_deduction_three_objects\": 3,\n",
        "    \"causal_judgement\": 2,\n",
        "    \"formal_fallacies\": 2,\n",
        "    \"tracking_shuffled_objects_seven_objects\": 7,\n",
        "    \"ruin_names\": 6,\n",
        "    \"penguins_in_a_table\": 5,\n",
        "    \"boolean_expressions\": 2,\n",
        "    \"tracking_shuffled_objects_five_objects\": 5\n",
        "}"
      ],
      "metadata": {
        "id": "RAE37WZiod3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "musr_subtasks = {\n",
        "    'murder_mysteries': 2,\n",
        "    'object_placements': 5,\n",
        "    'team_allocation': 3\n",
        "}"
      ],
      "metadata": {
        "id": "UZvE5kBJofy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and process results data"
      ],
      "metadata": {
        "id": "oGXAat1QdepR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the new_results.json file\n",
        "with open('/content/results_2024-09-23T09-45-37.626278.json', 'r') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "W7pG8vZgo0Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the model name and precision\n",
        "model_name = data['model_name']\n",
        "precision = data['config']['model_dtype']\n",
        "revision = data['config']['model_revision']"
      ],
      "metadata": {
        "id": "ZON320awkL1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize BBH subtasks scores\n",
        "bbh_scores = []\n",
        "for subtask, num_choices in bbh_subtasks.items():\n",
        "    subtask_key = f'leaderboard_bbh_{subtask}'\n",
        "    if subtask_key in data['results']:\n",
        "        bbh_raw_score = data['results'][subtask_key]['acc_norm,none']\n",
        "        lower_bound = 1 / num_choices\n",
        "        normalized_score = normalize_within_range(bbh_raw_score, lower_bound, 1.0)\n",
        "        bbh_scores.append(normalized_score)\n",
        "\n",
        "# Average BBH score\n",
        "bbh_score = sum(bbh_scores) / len(bbh_scores)"
      ],
      "metadata": {
        "id": "wcklqTbNkNZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the MATH score\n",
        "math_raw_score = data['results']['leaderboard_math_hard']['exact_match,none']\n",
        "math_score = normalize_within_range(math_raw_score, 0, 1.0)"
      ],
      "metadata": {
        "id": "xVpeButI8wl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize GPQA scores\n",
        "gpqa_raw_score = data['results']['leaderboard_gpqa']['acc_norm,none']\n",
        "gpqa_score = normalize_within_range(gpqa_raw_score, 0.25, 1.0)"
      ],
      "metadata": {
        "id": "MWfrIPcY8yHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize MMLU PRO scores\n",
        "mmlu_pro_raw_score = data['results']['leaderboard_mmlu_pro']['acc,none']\n",
        "mmlu_pro_score = normalize_within_range(mmlu_pro_raw_score, 0.1, 1.0)"
      ],
      "metadata": {
        "id": "yVTXRhs782fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute IFEval\n",
        "ifeval_inst_score = data['results']['leaderboard_ifeval']['inst_level_strict_acc,none'] * 100\n",
        "ifeval_prompt_score = data['results']['leaderboard_ifeval']['prompt_level_strict_acc,none'] * 100\n",
        "\n",
        "# Average IFEval scores\n",
        "ifeval_score = (ifeval_inst_score + ifeval_prompt_score) / 2"
      ],
      "metadata": {
        "id": "z7npCF8086XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Raw IFEval\n",
        "raw_inst_score = data['results']['leaderboard_ifeval']['inst_level_strict_acc,none']\n",
        "raw_prompt_score = data['results']['leaderboard_ifeval']['prompt_level_strict_acc,none']"
      ],
      "metadata": {
        "id": "X7NtA4YUDaNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize MUSR scores\n",
        "musr_scores = []\n",
        "\n",
        "for subtask, num_choices in musr_subtasks.items():\n",
        "    musr_raw_score = data['results'][f'leaderboard_musr_{subtask}']['acc_norm,none']\n",
        "    lower_bound = 1 / num_choices\n",
        "    normalized_score = normalize_within_range(musr_raw_score, lower_bound, 1.0)\n",
        "    musr_scores.append(normalized_score)\n",
        "\n",
        "musr_score = sum(musr_scores) / len(musr_scores)"
      ],
      "metadata": {
        "id": "vmEAdn979BCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate overall score\n",
        "overall_score = (bbh_score + math_score + gpqa_score + mmlu_pro_score + musr_score + ifeval_score) / 6"
      ],
      "metadata": {
        "id": "aOZz6Z6-lAK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round all scores to 2 decimal places\n",
        "bbh_score = round(bbh_score, 2)\n",
        "math_score = round(math_score, 2)\n",
        "gpqa_score = round(gpqa_score, 2)\n",
        "mmlu_pro_score = round(mmlu_pro_score, 2)\n",
        "musr_score = round(musr_score, 2)\n",
        "ifeval_score = round(ifeval_score, 2)\n",
        "overall_score = round(overall_score, 2)"
      ],
      "metadata": {
        "id": "9uMzZxaHnHEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalized results"
      ],
      "metadata": {
        "id": "S2pCOJyScmPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final results\n",
        "results = {\n",
        "    \"Model name\": model_name,\n",
        "    \"Precision\": precision,\n",
        "    \"Revision\": revision,\n",
        "    \"Average\": overall_score,\n",
        "    \"IFEval\": ifeval_score,\n",
        "    \"BBH\": bbh_score,\n",
        "    \"MATH Lvl 5\": math_score,\n",
        "    \"GPQA\": gpqa_score,\n",
        "    \"MUSR\": musr_score,\n",
        "    \"MMLU-PRO\": mmlu_pro_score\n",
        "}\n",
        "\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLRM2PhVlBhG",
        "outputId": "410c8cb9-3c21-4f1c-a293-43b4c4b31526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Model name': 'meta-llama/Llama-3.2-1B',\n",
              " 'Precision': 'torch.bfloat16',\n",
              " 'Revision': 'a7c18587d7f473bfea02aa5639aa349403307b54',\n",
              " 'Average': 4.03,\n",
              " 'IFEval': 14.78,\n",
              " 'BBH': 4.37,\n",
              " 'MATH Lvl 5': 0.23,\n",
              " 'GPQA': 0.0,\n",
              " 'MUSR': 2.56,\n",
              " 'MMLU-PRO': 2.26}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eATtjN2PPe_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}